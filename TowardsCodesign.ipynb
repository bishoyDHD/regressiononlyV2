{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10683b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import awkward as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77919377",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pionplus_1k.npy'\n",
    "file = open(filename, 'rb')\n",
    "mc_truth = np.load(file,allow_pickle=True) #Added Generator E and Angle\n",
    "data = np.load(file,allow_pickle=True)\n",
    "\n",
    "test_file = open('test.npy', 'rb')\n",
    "test_data = np.load(test_file,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9967b0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- ORIGINAL test.npy -------------------\n",
      "number of images in event 400\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------- ORIGINAL test.npy -------------------\")\n",
    "print('number of images in event', len(test_data[0]))\n",
    "#print(test_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f22c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- NEW .npy ----------------\n",
      "number of images in event 400\n",
      "(1000, 400)\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------- NEW .npy ----------------\")\n",
    "print('number of images in event', len(data[0]))\n",
    "#print(data[0][0])\n",
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0898d12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.007061"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_truth.item().get('true_energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a86edf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images in event 400\n",
      "(400, 96, 6)\n",
      "(400,)\n"
     ]
    }
   ],
   "source": [
    "#============ Original ============================\n",
    "X = []\n",
    "Y = []\n",
    "with open('test.npy', 'rb') as f:#wrong file for now\n",
    "#with open(filename, 'rb') as f:\n",
    "    data = np.load(f,allow_pickle=True)\n",
    "    ievt = 0\n",
    "    print('number of images in event', len(data[ievt]))\n",
    "    ## Here we loop over all \"images\", which are created by integrating HCAL sampling layers defing 3 sectors defined \n",
    "    ## by two z position that define boundary. Note for all images ECAL is the same (no longitudinal separation in ECAL)\n",
    "    for im in range(len(data[ievt])):\n",
    "        event = np.c_[data[ievt][im]['HCAL1_x'][0],data[ievt][im]['HCAL1_y'][0],data[ievt][im]['HCAL1_E'][0]]\n",
    "        depths = np.array([2*np.ones(len(data[ievt][im]['HCAL1_x'][0]))]).flatten()\n",
    "        if (len(data[ievt][im]['HCAL2_x'])>0):\n",
    "            event = np.concatenate([event,np.c_[data[ievt][im]['HCAL2_x'][0],data[ievt][im]['HCAL2_y'][0],data[ievt][im]['HCAL2_E'][0]]])\n",
    "            depths = np.concatenate([depths,np.array([3*np.ones(len(data[ievt][im]['HCAL2_x'][0]))]).flatten()])\n",
    "        if (len(data[ievt][im]['HCAL3_x'])>0):\n",
    "            event = np.concatenate([event,np.c_[data[ievt][im]['HCAL3_x'][0],data[ievt][im]['HCAL3_y'][0],data[ievt][im]['HCAL3_E'][0]]])\n",
    "            depths = np.concatenate([depths,np.array([4*np.ones(len(data[ievt][im]['HCAL3_x'][0]))]).flatten()])\n",
    "        event = np.concatenate([event,np.c_[data[ievt][im]['ECAL_x'],data[ievt][im]['ECAL_y'],data[ievt][im]['ECAL_E']]])\n",
    "        depths = np.concatenate([depths,np.array([1*np.ones(len(data[ievt][im]['ECAL_x']))]).flatten()])\n",
    "        event = np.insert(event, 3, depths,axis=1)\n",
    "        event = np.insert(event, 4, data[ievt][im]['boundary'][0]*np.ones(len(event)),axis=1) #better to make these global features, but I did not want to download the latest energyflow package\n",
    "        event = np.insert(event, 5, data[ievt][im]['boundary'][1]*np.ones(len(event)),axis=1)\n",
    "        X += [event]\n",
    "        trueenergy = 20.007061 #Miguel, please add this!\n",
    "        Y += [trueenergy]\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "print(np.shape(X))\n",
    "print(np.shape(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58943f55-0cd5-4cac-8eab-c293ceb5455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def awk_to_numpy_padded(awk_array,fill_val,ncell_max=1200,cell_axis=2,clip=True):\n",
    "\n",
    "    none_padded = ak.pad_none(ak.ArrayBuilder.snapshot(awk_array), ncell_max, axis=cell_axis, clip=True)\n",
    "    none_to_val = ak.fill_none(none_padded,fill_val,axis=cell_axis)\n",
    "    array = np.squeeze(ak.to_numpy(none_to_val))\n",
    "\n",
    "    return(array)\n",
    "\n",
    "ncell_max = 1200 #Root file indicates avg. of ~270 per ECal and HCal each. Safety X 2\n",
    "cell_axis = 2\n",
    "n_cell_variables = 6\n",
    "fill_val = np.zeros(n_cell_variables) #for zero paddin awkward array\n",
    "#fill_val = np.full([6], np.nan)\n",
    "#print(fill_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d562c02-39c6-4271-8e21-8868602fb8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Events: 1000 N Images per Event: 400\n",
      "ECAL X: (465,) + ECAL Y (465,) + ECAL E (465,) = (465, 3) [using np.c_]\n",
      "HCAL X: (1, 75) + HCAL Y (1, 75) + HCAL E (1, 75) =  -> np.append to ECAL -> (603, 3)\n"
     ]
    }
   ],
   "source": [
    "X_awk = ak.ArrayBuilder()\n",
    "X_awk.begin_list()\n",
    "Y = []\n",
    "\n",
    "with open(filename, 'rb') as f:\n",
    "#with open(\"test.npy\", 'rb') as f:\n",
    "    mc_truth = np.load(f,allow_pickle=True)\n",
    "    data = np.load(f,allow_pickle=True)\n",
    "    \n",
    "    print('N Events:', len(data),'N Images per Event:',len(data[0]))\n",
    "    \n",
    "    #ievt=4\n",
    "    #if not(ievt==999):\n",
    "    for ievt in range(0,len(data)):\n",
    "    #for ievt in range(0,1):\n",
    "        \n",
    "        ## Here we loop over all events and then all \"images\", which are created by integrating HCAL sampling layers defing 3 sectors defined \n",
    "        ## by two z position that define boundary. Note for all images ECAL is the same (no longitudinal separation in ECAL)\n",
    "\n",
    "        for im in range(len(data[ievt])):\n",
    "            #FIXME: assumes ECAL is always hit...\n",
    "            event = np.c_[data[ievt][im]['ECAL_x'],data[ievt][im]['ECAL_y'],data[ievt][im]['ECAL_E']]\n",
    "            depths = np.array([1*np.ones(len(data[ievt][im]['ECAL_x']))]).flatten()\n",
    "            \n",
    "            if (ievt == im == 0):\n",
    "                print(\"ECAL X:\", np.shape(data[ievt][im]['ECAL_x']), \"+ ECAL Y\", np.shape(data[ievt][im]['ECAL_y']),\n",
    "                  \"+ ECAL E\", np.shape(data[ievt][im]['ECAL_E']),\"=\",np.shape(event), \"[using np.c_]\")\n",
    "\n",
    "            if (len(data[ievt][im]['HCAL1_x'])>0):\n",
    "                event = np.concatenate([event,np.c_[data[ievt][im]['HCAL1_x'][0],data[ievt][im]['HCAL1_y'][0],data[ievt][im]['HCAL1_E'][0]]])\n",
    "                depths = np.concatenate([depths,np.array([2*np.ones(len(data[ievt][im]['HCAL1_x'][0]))]).flatten()])\n",
    "            \n",
    "            if (len(data[ievt][im]['HCAL2_x'])>0):\n",
    "                event = np.concatenate([event,np.c_[data[ievt][im]['HCAL2_x'][0],data[ievt][im]['HCAL2_y'][0],data[ievt][im]['HCAL2_E'][0]]])\n",
    "                depths = np.concatenate([depths,np.array([3*np.ones(len(data[ievt][im]['HCAL2_x'][0]))]).flatten()])\n",
    "                \n",
    "            if (len(data[ievt][im]['HCAL3_x'])>0):\n",
    "                event = np.concatenate([event,np.c_[data[ievt][im]['HCAL3_x'][0],data[ievt][im]['HCAL3_y'][0],data[ievt][im]['HCAL3_E'][0]]])\n",
    "                depths = np.concatenate([depths,np.array([4*np.ones(len(data[ievt][im]['HCAL3_x'][0]))]).flatten()])\n",
    "            \n",
    "            if (ievt == im == 0):\n",
    "                print(\"HCAL X:\", np.shape(data[ievt][im]['HCAL3_x']), \"+ HCAL Y\", np.shape(data[ievt][im]['HCAL3_y']), \n",
    "                      \"+ HCAL E\",np.shape(data[ievt][im]['HCAL3_E']),\"= \", \"-> np.append to ECAL ->\",np.shape(event))\n",
    "            \n",
    "            event = np.insert(event, 3, depths,axis=1)\n",
    "            event = np.insert(event, 4, data[ievt][im]['boundary'][0]*np.ones(len(event)),axis=1)\n",
    "            #FIXME: use num_global_features -- Number of additional features to be \n",
    "            #concatenated with the latent space observables to form the input to F.\n",
    "            event = np.insert(event, 5, data[ievt][im]['boundary'][1]*np.ones(len(event)),axis=1)\n",
    "            \n",
    "            X_awk.append(event)\n",
    "            trueenergy = mc_truth.item().get('true_energy') #TARGET\n",
    "            Y += [trueenergy]\n",
    "            \n",
    "X_awk.end_list()\n",
    "\n",
    "X = awk_to_numpy_padded(X_awk,fill_val,ncell_max,cell_axis)\n",
    "Y = np.array(Y)\n",
    "print(\"X shape =\",np.shape(X),\"([Images X Events][Cells][XYEDBB])\") #X,Y,Energy,Depth,Boundary,Boundary\n",
    "print(\"Y shape =\",np.shape(Y),\"[MC Truth Energy]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d789e8-39ba-46bb-a4ee-30e455e4ac8e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c256e3-8b19-4b01-a5c2-9470283f04d4",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.utils.normalize(X, axis=-1, order=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a986a88b-3a27-40a5-aa0c-4e1a77f1e3a1",
   "metadata": {},
   "source": [
    "for x in X:\n",
    "    xy_avg = np.average(x[:,0:2], axis=0)\n",
    "    x[:,0:2] -= xy_avg\n",
    "    x[:,2] /= 100. #could make this smarter\n",
    "    x[:,4:6] /= 100.\n",
    "#standardize entire input array. sklearn standard scalar. \n",
    "\n",
    "#QUESTION: What's the ultimate goal of this normalization? Can we use tensorflow built in normalization tools?\n",
    "print(\"NaNs in X data = \",np.any(np.isnan(X)))\n",
    "print(\"NaNs in Y data = \",np.any(np.isnan(Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaf6a59-76ce-4380-a8f9-d1284150c3b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Standardize input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948cc92c-eea0-4b6f-a727-2f10d320dc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0][0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff31609-cada-4ea4-ae1a-7848f4e4f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ultimately want an array with 6 numbers, containing the mean of each (XYEDBB)\n",
    "#probably easier to compute by hand [Images X Events][Cells][XYEDBB]\n",
    "#X_train_new = (X_train-np.mean(X_train))/np.stdev(X_train)\n",
    "import copy\n",
    "X_nan = copy.deepcopy(X) # otherwise, 0 get's replaced with nan's...\n",
    "X_nan[X_nan ==0] = np.nan\n",
    "\n",
    "X_mean = np.nanmean(X_nan,axis=(0,1))\n",
    "X_std = np.nanstd(X_nan,axis=(0,1))\n",
    "\n",
    "print( np.shape(X_nan), np.shape(X_mean)[0], np.shape(X_std)[0] )\n",
    "print( X_mean )\n",
    "print( X_std )    \n",
    "print(X_mean/X_std)\n",
    "Test_X = (X-X_mean)/X_std\n",
    "#Change original X dateset to be filled with NaN (or None), do this calculation, including stdev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4585978-ccca-4b37-81d5-849a65a278ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X[0][0][3])\n",
    "#print(Test_X[0][0][3])\n",
    "#Test_X = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799d9e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_val, X_test,\n",
    " Y_train, Y_val, Y_test) = data_split(Test_X, Y, val=0.2, test=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0e3ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Phi_sizes, F_sizes = (100, 100, 128), (100, 100, 100)\n",
    "output_act, output_dim = 'linear', 1\n",
    "loss = 'mse' #mean-squared error\n",
    "pfn = PFN(input_dim=X.shape[-1], Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "          output_act=output_act, output_dim=output_dim, loss=loss, optimizer=tf.keras.optimizers.Adam(learning_rate=0.1))\n",
    "          #output_act=output_act, output_dim=output_dim, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfe386b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "the_fit = pfn.fit(X_train, Y_train,\n",
    "                  epochs=100,\n",
    "                  batch_size=100,\n",
    "                  validation_data=(X_val, Y_val),\n",
    "                  verbose=1)\n",
    "\n",
    "#myfit = pfn.fit...\n",
    "#plot loss vs epoch #\n",
    "#if val_loss goes up, overfitting. tf.earlystopping can prevent (call_back -- monitors or interevens during training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b0b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfn.layers\n",
    "mypreds = pfn.predict(X_test,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f69ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,10))\n",
    "plt.scatter(Y_test,mypreds)\n",
    "plt.xlabel(\"Y Test [GeV]\",fontsize=22)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.ylabel(\"Y Pred [GeV]\",fontsize=22)\n",
    "plt.title(\"Prediction vs. Test\",fontsize=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a30507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "plt.plot(the_fit.history['loss'])\n",
    "plt.plot(the_fit.history['val_loss'])\n",
    "plt.title('Model Loss vs. Epoch',fontsize=26)\n",
    "plt.ylabel('Mean-Squared Error',fontsize=22)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('epoch',fontsize=22)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlim([-1,101])\n",
    "plt.legend(['train', 'validation'], loc='upper right',fontsize=22)\n",
    "plt.show()\n",
    "\n",
    "#print(Y_test)\n",
    "#print(mypreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eedab83-fcdf-4063-b35d-1ea0a6c346a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
