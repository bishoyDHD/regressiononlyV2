{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10683b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import awkward as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77919377",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pionplus_1k.npy'\n",
    "file = open(filename, 'rb')\n",
    "mc_truth = np.load(file,allow_pickle=True) #Added Generator E and Angle\n",
    "data = np.load(file,allow_pickle=True)\n",
    "\n",
    "test_file = open('test.npy', 'rb')\n",
    "test_data = np.load(test_file,allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9967b0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- ORIGINAL test.npy -------------------\n",
      "number of images in event 400\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------- ORIGINAL test.npy -------------------\")\n",
    "print('number of images in event', len(test_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f22c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- NEW .npy ----------------\n",
      "number of images in event 400\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------- NEW .npy ----------------\")\n",
    "print('number of images in event', len(data[0]))\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0898d12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.007061"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_truth.item().get('true_energy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a86edf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images in event 400\n",
      "(400, 96, 6)\n"
     ]
    }
   ],
   "source": [
    "#============ Original ============================\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "with open('test.npy', 'rb') as f:#wrong file for now\n",
    "#with open(filename, 'rb') as f:\n",
    "    data = np.load(f,allow_pickle=True)\n",
    "    ievt = 0\n",
    "    print('number of images in event', len(data[ievt]))\n",
    "    ## Here we loop over all \"images\", which are created by integrating HCAL sampling layers defing 3 sectors defined \n",
    "    ## by two z position that define boundary. Note for all images ECAL is the same (no longitudinal separation in ECAL)\n",
    "    for im in range(len(data[ievt])):\n",
    "        event = np.c_[data[ievt][im]['HCAL1_x'][0],data[ievt][im]['HCAL1_y'][0],data[ievt][im]['HCAL1_E'][0]]\n",
    "        depths = np.array([2*np.ones(len(data[ievt][im]['HCAL1_x'][0]))]).flatten()\n",
    "        if (len(data[ievt][im]['HCAL2_x'])>0):\n",
    "            event = np.concatenate([event,np.c_[data[ievt][im]['HCAL2_x'][0],data[ievt][im]['HCAL2_y'][0],data[ievt][im]['HCAL2_E'][0]]])\n",
    "            depths = np.concatenate([depths,np.array([3*np.ones(len(data[ievt][im]['HCAL2_x'][0]))]).flatten()])\n",
    "        if (len(data[ievt][im]['HCAL3_x'])>0):\n",
    "            event = np.concatenate([event,np.c_[data[ievt][im]['HCAL3_x'][0],data[ievt][im]['HCAL3_y'][0],data[ievt][im]['HCAL3_E'][0]]])\n",
    "            depths = np.concatenate([depths,np.array([4*np.ones(len(data[ievt][im]['HCAL3_x'][0]))]).flatten()])\n",
    "        event = np.concatenate([event,np.c_[data[ievt][im]['ECAL_x'],data[ievt][im]['ECAL_y'],data[ievt][im]['ECAL_E']]])\n",
    "        depths = np.concatenate([depths,np.array([1*np.ones(len(data[ievt][im]['ECAL_x']))]).flatten()])\n",
    "        event = np.insert(event, 3, depths,axis=1)\n",
    "        event = np.insert(event, 4, data[ievt][im]['boundary'][0]*np.ones(len(event)),axis=1) #better to make these global features, but I did not want to download the latest energyflow package\n",
    "        event = np.insert(event, 5, data[ievt][im]['boundary'][1]*np.ones(len(event)),axis=1)\n",
    "        X += [event]\n",
    "        trueenergy = 1. #Miguel, please add this!\n",
    "        Y += [trueenergy]\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58943f55-0cd5-4cac-8eab-c293ceb5455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def awk_to_numpy_padded(awk_array,fill_val,ncell_max=1200,cell_axis=2,clip=True):\n",
    "\n",
    "    none_padded = ak.pad_none(ak.ArrayBuilder.snapshot(awk_array), ncell_max, axis=cell_axis, clip=True)\n",
    "    none_to_val = ak.fill_none(none_padded,fill_val,axis=cell_axis)\n",
    "    array = np.squeeze(ak.to_numpy(none_to_val))\n",
    "\n",
    "    print(\"new numpy shape =\",np.shape(array))\n",
    "    return(array)\n",
    "\n",
    "ncell_max = 1200 #Root file indicase avg. of ~270 per ECal and HCal each. \n",
    "cell_axis = 2\n",
    "n_cell_variables = 6\n",
    "fill_val = np.zeros(n_cell_variables) #for zero paddin awkward array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3b832c3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Events: 1000 N Images per Event: 400\n",
      "ECAL X: (311,) + ECAL Y (311,) + ECAL E (311,) = (311, 3) [using np.c_]\n",
      "HCAL X: (1, 169) + HCAL Y (1, 169) + HCAL E (1, 169) =  -> np.append to ECAL -> (591, 3)\n",
      "new numpy shape = (8000, 1200, 6)\n",
      "X shape = (8000, 1200, 6) ([Images X Events][Cells][XYEDBB])\n",
      "Y shape = (8000,) [MC Truth Energy]\n"
     ]
    }
   ],
   "source": [
    "X_awk = ak.ArrayBuilder()\n",
    "X_awk.begin_list()\n",
    "Y = []\n",
    "\n",
    "with open(filename, 'rb') as f:\n",
    "#with open(\"test.npy\", 'rb') as f:\n",
    "    mc_truth = np.load(f,allow_pickle=True)\n",
    "    data = np.load(f,allow_pickle=True)\n",
    "    \n",
    "    print('N Events:', len(data),'N Images per Event:',len(data[0]))\n",
    "    \n",
    "    ievt=0\n",
    "    #for ievt in range(0,len(data)):\n",
    "    for ievt in range(0,20):\n",
    "        \n",
    "        ## Here we loop over all events and then all \"images\", which are created by integrating HCAL sampling layers defing 3 sectors defined \n",
    "        ## by two z position that define boundary. Note for all images ECAL is the same (no longitudinal separation in ECAL)\n",
    "\n",
    "        for im in range(len(data[ievt])):\n",
    "            #FIXME: assumes ECAL is always hit...\n",
    "            event = np.c_[data[ievt][im]['ECAL_x'],data[ievt][im]['ECAL_y'],data[ievt][im]['ECAL_E']]\n",
    "            depths = np.array([1*np.ones(len(data[ievt][im]['ECAL_x']))]).flatten()\n",
    "            \n",
    "            if (ievt == im == 0):\n",
    "                print(\"ECAL X:\", np.shape(data[ievt][im]['ECAL_x']), \"+ ECAL Y\", np.shape(data[ievt][im]['ECAL_y']),\n",
    "                  \"+ ECAL E\", np.shape(data[ievt][im]['ECAL_E']),\"=\",np.shape(event), \"[using np.c_]\")\n",
    "\n",
    "            if (len(data[ievt][im]['HCAL1_x'])>0):\n",
    "                event = np.concatenate([event,np.c_[data[ievt][im]['HCAL1_x'][0],data[ievt][im]['HCAL1_y'][0],data[ievt][im]['HCAL1_E'][0]]])\n",
    "                depths = np.concatenate([depths,np.array([2*np.ones(len(data[ievt][im]['HCAL1_x'][0]))]).flatten()])\n",
    "            \n",
    "            if (len(data[ievt][im]['HCAL2_x'])>0):\n",
    "                event = np.concatenate([event,np.c_[data[ievt][im]['HCAL2_x'][0],data[ievt][im]['HCAL2_y'][0],data[ievt][im]['HCAL2_E'][0]]])\n",
    "                depths = np.concatenate([depths,np.array([3*np.ones(len(data[ievt][im]['HCAL2_x'][0]))]).flatten()])\n",
    "                \n",
    "            if (len(data[ievt][im]['HCAL3_x'])>0):\n",
    "                event = np.concatenate([event,np.c_[data[ievt][im]['HCAL3_x'][0],data[ievt][im]['HCAL3_y'][0],data[ievt][im]['HCAL3_E'][0]]])\n",
    "                depths = np.concatenate([depths,np.array([4*np.ones(len(data[ievt][im]['HCAL3_x'][0]))]).flatten()])\n",
    "            \n",
    "            if (ievt == im == 0):\n",
    "                print(\"HCAL X:\", np.shape(data[ievt][im]['HCAL3_x']), \"+ HCAL Y\", np.shape(data[ievt][im]['HCAL3_y']), \n",
    "                      \"+ HCAL E\",np.shape(data[ievt][im]['HCAL3_E']),\"= \", \"-> np.append to ECAL ->\",np.shape(event))\n",
    "            \n",
    "            event = np.insert(event, 3, depths,axis=1)\n",
    "            event = np.insert(event, 4, data[ievt][im]['boundary'][0]*np.ones(len(event)),axis=1)\n",
    "            #FIXME: use num_global_features -- Number of additional features to be \n",
    "            #concatenated with the latent space observables to form the input to F.\n",
    "            event = np.insert(event, 5, data[ievt][im]['boundary'][1]*np.ones(len(event)),axis=1)\n",
    "            \n",
    "            X_awk.append(event)\n",
    "            trueenergy = mc_truth.item().get('true_energy') #TARGET\n",
    "            Y += [trueenergy]\n",
    "            \n",
    "X_awk.end_list()\n",
    "\n",
    "X = awk_to_numpy_padded(X_awk,fill_val,ncell_max,cell_axis)\n",
    "Y = np.array(Y)\n",
    "print(\"X shape =\",np.shape(X),\"([Images X Events][Cells][XYEDBB])\") #X,Y,Energy,Depth,Boundary,Boundary\n",
    "print(\"Y shape =\",np.shape(Y),\"[MC Truth Energy]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d789e8-39ba-46bb-a4ee-30e455e4ac8e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e5d2637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import energyflow as ef\n",
    "from energyflow.archs import PFN\n",
    "from energyflow.utils import data_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c256e3-8b19-4b01-a5c2-9470283f04d4",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.utils.normalize(X, axis=-1, order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a37636-bb9f-461b-ac04-a57e03a9bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X:\n",
    "    xy_avg = np.average(x[:,0:2], axis=0)\n",
    "    x[:,0:2] -= xy_avg\n",
    "    x[:,2] /= 100. #could make this smarter\n",
    "    x[:,4:6] /= 100.\n",
    "    \n",
    "#QUESTION: What's the ultimate goal of this normalization? Can we use tensorflow built in normalization tools?\n",
    "print(\"NaNs in X data = \",np.any(np.isnan(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "799d9e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_val, X_test,\n",
    " Y_train, Y_val, Y_test) = data_split(X, Y, val=0.2, test=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ed65355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably want to standardize the input and output energies, or at least put them in units where the mean is O(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f0e3ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, None, 6)]    0           []                               \n",
      "                                                                                                  \n",
      " tdist_0 (TimeDistributed)      (None, None, 100)    700         ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, None, 100)    0           ['tdist_0[0][0]']                \n",
      "                                                                                                  \n",
      " tdist_1 (TimeDistributed)      (None, None, 100)    10100       ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, None, 100)    0           ['tdist_1[0][0]']                \n",
      "                                                                                                  \n",
      " tdist_2 (TimeDistributed)      (None, None, 128)    12928       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " mask (Lambda)                  (None, None)         0           ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, None, 128)    0           ['tdist_2[0][0]']                \n",
      "                                                                                                  \n",
      " sum (Dot)                      (None, 128)          0           ['mask[0][0]',                   \n",
      "                                                                  'activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " dense_0 (Dense)                (None, 100)          12900       ['sum[0][0]']                    \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 100)          0           ['dense_0[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 100)          10100       ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 100)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 100)          10100       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 100)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 1)            101         ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 1)            0           ['output[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 56,929\n",
      "Trainable params: 56,929\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Phi_sizes, F_sizes = (100, 100, 128), (100, 100, 100)\n",
    "output_act, output_dim = 'linear', 1\n",
    "loss = 'mse' #mean-squared error\n",
    "pfn = PFN(input_dim=X.shape[-1], Phi_sizes=Phi_sizes, F_sizes=F_sizes, \n",
    "          output_act=output_act, output_dim=output_dim, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfe386b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-18 10:45:46.100639: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 5s 109ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "40/40 [==============================] - 4s 106ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "40/40 [==============================] - 4s 106ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "40/40 [==============================] - 4s 107ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "40/40 [==============================] - 4s 107ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "40/40 [==============================] - 4s 107ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "40/40 [==============================] - 4s 108ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "40/40 [==============================] - 4s 108ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "40/40 [==============================] - 4s 109ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "40/40 [==============================] - 4s 109ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "40/40 [==============================] - 4s 109ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "40/40 [==============================] - 4s 109ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "40/40 [==============================] - 4s 109ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "40/40 [==============================] - 4s 110ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "40/40 [==============================] - 4s 111ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "40/40 [==============================] - 4s 110ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "40/40 [==============================] - 4s 111ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "40/40 [==============================] - 4s 111ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "40/40 [==============================] - 4s 111ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "40/40 [==============================] - 4s 112ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "40/40 [==============================] - 4s 112ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "40/40 [==============================] - 4s 112ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "40/40 [==============================] - 4s 112ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "40/40 [==============================] - 4s 113ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "40/40 [==============================] - 4s 113ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "40/40 [==============================] - 4s 113ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "40/40 [==============================] - 5s 113ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "40/40 [==============================] - 5s 114ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "40/40 [==============================] - 5s 113ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "40/40 [==============================] - 5s 114ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "40/40 [==============================] - 5s 114ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "40/40 [==============================] - 5s 115ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "40/40 [==============================] - 5s 118ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "40/40 [==============================] - 5s 117ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "40/40 [==============================] - 5s 116ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "40/40 [==============================] - 5s 116ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "40/40 [==============================] - 5s 117ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "40/40 [==============================] - 5s 119ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "40/40 [==============================] - 5s 120ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "40/40 [==============================] - 5s 122ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "40/40 [==============================] - 5s 126ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "40/40 [==============================] - 5s 134ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "40/40 [==============================] - 5s 130ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "40/40 [==============================] - 5s 130ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "40/40 [==============================] - 5s 130ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "40/40 [==============================] - 5s 132ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "40/40 [==============================] - 5s 133ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "40/40 [==============================] - 5s 135ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "40/40 [==============================] - 5s 136ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      "40/40 [==============================] - 6s 139ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "40/40 [==============================] - 6s 141ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "40/40 [==============================] - 6s 143ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "40/40 [==============================] - 6s 146ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "40/40 [==============================] - 6s 148ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      "40/40 [==============================] - 6s 150ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "40/40 [==============================] - 6s 151ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      "40/40 [==============================] - 6s 154ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      "40/40 [==============================] - 6s 155ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      "40/40 [==============================] - 6s 158ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "40/40 [==============================] - 6s 163ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "40/40 [==============================] - 7s 167ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "40/40 [==============================] - 7s 169ms/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "36/40 [==========================>...] - ETA: 0s - loss: nan - acc: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "pfn.fit(X_train, Y_train,\n",
    "        epochs=100,\n",
    "        batch_size=100,\n",
    "        validation_data=(X_val, Y_val),\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b0b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfn.layers\n",
    "mypreds = pfn.predict(X_test,batch_size=100)\n",
    "print(np.shape(mypreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f69ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_test,mypreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c86246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#????????!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a30507",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_test)\n",
    "print(mypreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2551c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def awk_to_numpy_padded(fill_val,ncell_max=1200,cell_axis=2,clip=True):\n",
    "\n",
    "    none_padded = ak.pad_none(ak.ArrayBuilder.snapshot(X), ncell_max, axis=cell_axis, clip=True)\n",
    "    none_to_val = ak.fill_none(none_padded,fill_val,axis=cell_axis)\n",
    "    array = np.squeeze(ak.to_numpy(none_to_0))\n",
    "\n",
    "    print(\"new numpy shape =\",np.shape(array))\n",
    "    return(array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
